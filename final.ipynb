{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting distribution shifts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(0)\n",
    "from scipy.optimize import fsolve\n",
    "from scipy.special import erf\n",
    "from scipy.integrate import quad\n",
    "from functools import partial\n",
    "from scipy.stats import binomtest\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_openml\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "\n",
    "from crepes import ConformalRegressor, ConformalPredictiveSystem\n",
    "\n",
    "from crepes.fillings import (sigma_variance, \n",
    "                            sigma_variance_oob,\n",
    "                            sigma_knn,\n",
    "                            binning)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we explain how to implement the shift detection algorithm. We start downloading the data and modeling it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we load the dataset\n",
    "dataset = fetch_openml(name=\"house_sales\",version=3)\n",
    "\n",
    "X = dataset.data.values.astype(float)\n",
    "y = dataset.target.values.astype(float)\n",
    "print('Before normalization range:',y.min(), y.max())\n",
    "\n",
    "# and also normalize it such that the y values are in the range [0,1]\n",
    "#y = (np.tanh(np.array([(y[i]-y.min())/(y.max()-y.min()) for i in range(len(y))])*2-1)+1)/2.\n",
    "#y = np.array([(y[i]-y.min())/(y.max()-y.min()) for i in range(len(y))])\n",
    "y = np.tanh(y/y.mean())\n",
    "print('After normalization range:',y.min(), y.max())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we spit the data in the main and shifted data, and form tuples for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then we split the data such that we can later on introduce a distribution shift\n",
    "X_main, X_shift, y_main, y_shift = X[(4e4 >= X[:, 3])] , X[(4e4 < X[:, 3])] , y[(4e4 >= X[:, 3])] , y[(4e4 < X[:, 3])]\n",
    "\n",
    "# We want to have an even number of samples, to create tuples\n",
    "if len(X_shift) % 2 == 1:\n",
    "    X_shift = X_shift[:-1]\n",
    "    y_shift = y_shift[:-1]\n",
    "if len(X_main) % 2 == 1:\n",
    "    X_main = X_main[:-1]\n",
    "    y_main = y_main[:-1]\n",
    "\n",
    "## The input will be two copies of X, one for each house\n",
    "X_main = X_main.reshape(X_main.shape[0]//2,-1)\n",
    "X_shift = X_shift.reshape(X_shift.shape[0]//2,-1)\n",
    "print('After creating tuples',X_main.shape, X_shift.shape)\n",
    "\n",
    "## We want to estimate the difference between the two y values\n",
    "y_main = y_main.reshape(y_main.shape[0]//2,-1)\n",
    "y_main = y_main[:,0] - y_main[:,1]\n",
    "print(y_main.shape)\n",
    "\n",
    "## and also in the shifted dataset\n",
    "y_shift = y_shift.reshape(y_shift.shape[0]//2,-1)\n",
    "y_shift = y_shift[:,0] - y_shift[:,1]\n",
    "print(y_shift.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we create a calibration dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_cal, y_train, y_cal = train_test_split(X_main, y_main, test_size=500)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create the model and train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then we train a random forest regressor on the training set\n",
    "random_forest_model = RandomForestRegressor(n_jobs=-1, n_estimators=500) \n",
    "random_forest_model.fit(X_train, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conformal model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will target a success probability $1-\\delta = 0.95$. Then we train the conformal model on the residuals on the predictions of the random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = 0.05\n",
    "\n",
    "cr_std = ConformalRegressor()\n",
    "y_hat_cal = random_forest_model.predict(X_cal)\n",
    "residuals_cal = y_cal - y_hat_cal\n",
    "# and fit it to the residuals\n",
    "cr_std.fit(residuals=residuals_cal)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now make prediction on a given test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using such model we can now predict the residuals on the test set\n",
    "y_hat_shift = random_forest_model.predict(X_shift)\n",
    "intervals_std = cr_std.predict(y_hat=y_hat_shift, confidence=1-delta)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional -- More complex models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous models assigns the same uncertainty interval to all predictions. We can alternatively use two more models that callibrate the confidence interval for each prediction based on their heuristic difficulty using k-nearest neighbours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second model based on k-nn\n",
    "sigmas_cal_knn = sigma_knn(X=X_cal, residuals=residuals_cal)\n",
    "cr_norm_knn = ConformalRegressor()\n",
    "cr_norm_knn.fit(residuals=residuals_cal, sigmas=sigmas_cal_knn)\n",
    "sigmas_test_knn = sigma_knn(X=X_cal, residuals=residuals_cal, X_test=X_shift)\n",
    "intervals_norm_knn = cr_norm_knn.predict(y_hat=y_hat_shift, \n",
    "                                        sigmas=sigmas_test_knn,\n",
    "                                        y_min=0, y_max=1)\n",
    "\n",
    "# Third model based on binning\n",
    "bins_cal, bin_thresholds = binning(values=sigmas_cal_knn, bins=20)\n",
    "cr_mond = ConformalRegressor()\n",
    "cr_mond.fit(residuals=residuals_cal, bins=bins_cal)\n",
    "bins_test = binning(values=sigmas_test_knn, bins=bin_thresholds)\n",
    "intervals_mond = cr_mond.predict(y_hat=y_hat_shift, bins=bins_test, \n",
    "                                        y_min=0, y_max=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compute the coverage of the three models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverages = []\n",
    "mean_sizes = []\n",
    "median_sizes = []\n",
    "\n",
    "prediction_intervals = {\n",
    "    \"Std CR\":intervals_std,\n",
    "    \"Norm CR knn\":intervals_norm_knn,\n",
    "    \"Mond CR\":intervals_mond,\n",
    "}\n",
    "\n",
    "for name in prediction_intervals.keys():\n",
    "    intervals = prediction_intervals[name]\n",
    "    coverages.append(np.sum([1 if (y_shift[i]>=intervals[i,0] and \n",
    "                                   y_shift[i]<=intervals[i,1]) else 0 \n",
    "                            for i in range(len(y_shift))])/len(y_shift))\n",
    "    mean_sizes.append((intervals[:,1]-intervals[:,0]).mean())\n",
    "    median_sizes.append(np.median((intervals[:,1]-intervals[:,0])))\n",
    "\n",
    "pred_int_df = pd.DataFrame({\"Coverage\":coverages, \n",
    "                            \"Mean size\":mean_sizes, \n",
    "                            \"Median size\":median_sizes}, \n",
    "                           index=list(prediction_intervals.keys()))\n",
    "\n",
    "pred_int_df.loc[\"Mean\"] = [pred_int_df[\"Coverage\"].mean(), \n",
    "                           pred_int_df[\"Mean size\"].mean(),\n",
    "                           pred_int_df[\"Median size\"].mean()]\n",
    "\n",
    "display(pred_int_df.round(4))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auxiliary functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous section we have used pre-defined conformal models to compute the confidence intervals. We can also assume gaussian intervals derived from these residuals and reverse engineer the confidence intervals."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we define function that computes the standard deviation from an interval and $1-\\delta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sigma(delta, initial_interval, mean):\n",
    "    \"\"\" Computes the standard deviation of a normal distribution such that the probability of the interval is 1-delta\"\"\"\n",
    "    def cdf(sigma_):\n",
    "        return 0.5*(erf((initial_interval[1]-mean)/(np.sqrt(2)*sigma_))-erf((initial_interval[0]-mean)/(np.sqrt(2)*sigma_))) - (1-delta)\n",
    "\n",
    "    sigma = fsolve(cdf, (initial_interval[1]-initial_interval[0])/2)[0]\n",
    "    return sigma"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we define a function to compute the interval in which the (assumed normal) probability distribution is larger than $\\alpha$. In other words, $\\mathcal{C}_\\alpha$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confidence_interval(mean, sigma, alpha, initial_interval_guess):\n",
    "    \"\"\" We want to compute the conformal integral in which the probability density is larger than alpha. \"\"\"\n",
    "    def normal_distribution(x, mean, sigma):\n",
    "        return np.exp(-((x-mean)**2)/(2*sigma**2))\n",
    "\n",
    "    interval_alpha = fsolve(lambda x: normal_distribution(x, mean, sigma) - alpha, x0 = initial_interval_guess)\n",
    "    return interval_alpha"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we define a function to integrate the loss on a given interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(y_hat, y_gt, sigma, interval_alpha):\n",
    "    \"\"\" Integrates the  quadratic loss over the interval defined by the probability density being larger than alpha.\"\"\"\n",
    "    def normal_loss(x, y, mean, sigma):\n",
    "        return np.exp(-((x-mean)**2)/(2*sigma**2))*np.abs(x-y)\n",
    "\n",
    "    normal_l = partial(normal_loss, y=y_gt, mean=y_hat, sigma = sigma)\n",
    "    loss = quad(normal_l, interval_alpha[0], interval_alpha[1])[0]\n",
    "    return loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to compute the p-value of the hypothesis $\\mathcal{H}_\\alpha$ that the risk $R(\\alpha)\\leq \\lambda$ for a chosen $\\lambda$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_ = 0.5\n",
    "\n",
    "def pvalue(y_gt, y_hat, sigma, lambda_):\n",
    "    \"\"\" Computes the p-value of the hypothesis that the risk is lower than some \\lambda.\"\"\"\n",
    "\n",
    "    # For each y compute confidence interval first compute the loss\n",
    "    alpha = np.linspace(0.01, 0.99, 100)\n",
    "    interval_alpha = confidence_interval(mean = y_hat, sigma = sigma, alpha = alpha, initial_interval_guess=[y_hat - sigma, y_hat+sigma])\n",
    "    loss = compute_loss(y_hat, y_gt, sigma, interval_alpha)\n",
    "\n",
    "    # Step 1: Compute p-values\n",
    "    pvalues = np.exp(-2*len(y_hat)*(lambda_ - loss)**2)\n",
    "\n",
    "    # Step 2: Family-wise error correction\n",
    "    reject, pvals_corrected, _, bonferroni_delta = multipletests(pvalues, delta, method = 'bonferroni')\n",
    "\n",
    "    return pvals_corrected, reject"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full conformal prediction and inductive conformal predictors\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to compute the loss of exchanging one element of the training set with the shifted data. Then we will compute the corresponding p-values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_x = np.expand_dims(X_shift[0], axis = 0)\n",
    "new_y = np.expand_dims(y_shift[0], axis = 0)\n",
    "\n",
    "alpha = 1e-5\n",
    "ncal = len(y_cal)\n",
    "\n",
    "# First we fit a model to the data\n",
    "random_forest_model = RandomForestRegressor(n_estimators=100)\n",
    "random_forest_model.fit(X_train, y_train)\n",
    "\n",
    "def find_lamdas(new_x, new_y, delta, random_forest_model: RandomForestRegressor, X_cal, y_cal):\n",
    "    lambdas_i = []\n",
    "    ncal = len(X_cal)\n",
    "    for i in range(ncal + 1):\n",
    "        if i < ncal:\n",
    "            j = np.random.randint(0, len(X_cal))\n",
    "        \n",
    "            X_cal_j = np.concatenate([X_cal[:j],new_x, X_cal[j+1:]])\n",
    "            y_cal_j = np.concatenate([y_cal[:j],new_y, y_cal[j+1:]])\n",
    "\n",
    "            X_test_j = np.expand_dims(X_cal[j], axis = 0)\n",
    "            y_test_j = np.expand_dims(y_cal[j], axis = 0)\n",
    "\n",
    "        else:\n",
    "            X_cal_j = X_cal\n",
    "            y_cal_j = y_cal\n",
    "\n",
    "            X_test_j = new_x\n",
    "            y_test_j = new_y\n",
    "\n",
    "        # We can now compute the residuals\n",
    "        y_hat_cal_j = random_forest_model.predict(X_cal_j)\n",
    "        residuals_cal = y_cal_j - y_hat_cal_j\n",
    "\n",
    "        # We can now fit a model to the residuals\n",
    "        cr_std = ConformalRegressor()\n",
    "        cr_std.fit(residuals=residuals_cal)\n",
    "\n",
    "        # Compute confidence intervals on sample X_train[j]\n",
    "        y_hat_j = random_forest_model.predict(X_test_j)\n",
    "        interval_j_std = cr_std.predict(y_hat=y_hat_j, confidence=1-delta)[0]\n",
    "\n",
    "        # Compute loss\n",
    "        sigma = compute_sigma(delta = delta, initial_interval = interval_j_std, mean = y_hat_j)\n",
    "        #interval_alpha = confidence_interval(mean = y_hat_j, sigma = sigma, alpha = alpha, initial_interval_guess = interval_j_std)\n",
    "        interval_alpha = interval_j_std\n",
    "        lambda_i = compute_loss(y_hat_j, y_test_j, sigma, interval_alpha)\n",
    "\n",
    "        # Compute lambda_i\n",
    "        lambdas_i.append(lambda_i)\n",
    "\n",
    "        # Save the last sigma, corresponding to the new sample, which we will use later on.\n",
    "        if i == ncal: \n",
    "            sigma_new = sigma\n",
    "            interval_alpha_new = interval_alpha\n",
    "\n",
    "    lambdas_i, lambda_y = lambdas_i[:-1], lambdas_i[-1]\n",
    "\n",
    "    return lambdas_i, lambda_y, sigma_new, interval_alpha_new\n",
    "\n",
    "lambdas_i, lambda_y, sigma_new, interval_alpha_new = find_lamdas(new_x, new_y, delta, random_forest_model, X_cal, y_cal)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to compute the possible values of y such that the corresponding p-value is larger than $\\epsilon$, $p^y \\geq \\epsilon$ for \n",
    "$$p^y := \\frac{|\\{ i \\in 0,\\ldots,n_{cal}| \\lambda_i \\leq \\lambda_y\\}|+1}{n_{cal}+1}\\geq \\epsilon.$$\n",
    "First we compute how the p-value should be\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.05\n",
    "lambda_y_lower_bound = np.quantile(lambdas_i, (epsilon*(ncal + 1) - 1)/len(lambdas_i))\n",
    "if lambda_y < lambda_y_lower_bound:\n",
    "    print(\"We detect distribution shift, based on the p-value above\")\n",
    "else:\n",
    "    print(\"We do not detect distribution shift, based on the p-value above\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to find the values of y that satisfies such bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_loss_partial = partial(compute_loss, y_gt = new_y, sigma = sigma_new, interval_alpha = interval_alpha_new)\n",
    "\n",
    "y_lower_bound = fsolve(lambda x: compute_loss_partial(x) - lambda_y_lower_bound, x0 = new_y-0.5)\n",
    "y_upper_bound = fsolve(lambda x: compute_loss_partial(x) - lambda_y_lower_bound, x0 = new_y+0.5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice we want to compute, for each value of x in the shifted dataset, the corresponding bounds on y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bounds(x, y, epsilon, delta, delta_prime, X_cal, Y_cal, model):\n",
    "    \"\"\" Computes the bounds on y for a given x.\"\"\"\n",
    "    # Compute the lambda_i values in the full conformal model\n",
    "    find_lamdas_partial = partial(find_lamdas, delta = delta, random_forest_model = random_forest_model, X_cal = X_cal, y_cal = y_cal)\n",
    "    lambdas_i, lambda_y, sigma_new, interval_alpha_new = find_lamdas_partial(x, y)\n",
    "\n",
    "    # Compute the lower bound on lambda_y\n",
    "    ncal = len(X_cal)\n",
    "    epsilon = epsilon + np.sqrt(-np.log(delta_prime)/(2*ncal))\n",
    "    lambda_y_lower_bound = np.quantile(lambdas_i, (epsilon*(ncal + 1) - 1)/len(lambdas_i))\n",
    "\n",
    "    # Compute the bounds on y\n",
    "    compute_loss_partial = partial(compute_loss, y_gt = y, sigma = sigma_new, interval_alpha = interval_alpha_new)\n",
    "    y_lower_bound = fsolve(lambda x: compute_loss_partial(x) - lambda_y_lower_bound, x0 = y-0.5) #todo: this does not seem to work very well\n",
    "    y_upper_bound = fsolve(lambda x: compute_loss_partial(x) - lambda_y_lower_bound, x0 = y+0.5)\n",
    "\n",
    "    return y_lower_bound, y_upper_bound\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can check whether the shifted dataset is in the confidence interval of the full conformal model with probability $1-\\epsilon$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_shift, y_shift = X_shift[:10], y_shift[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "cummulative = 0\n",
    "delta_prime = 1/2**4 # This is the delta in the definition of the inductive conformal predictor.\n",
    "delta = 1/2**4 # This is the delta used in the for the estimation of uncertainty intervals in the conformal model, to integrate the loss\n",
    "\n",
    "for new_x, new_y in tqdm(zip(X_shift, y_shift)):\n",
    "    new_x, new_y = np.expand_dims(new_x, axis = 0), np.expand_dims(new_y, axis = 0)\n",
    "    lower_bound, upper_bound = compute_bounds(new_x, new_y, epsilon, delta, delta_prime, X_cal, y_cal, random_forest_model)\n",
    "    if new_y > lower_bound and new_y < upper_bound: cummulative += 1\n",
    "\n",
    "cummulative /= len(X_shift)\n",
    "if cummulative > 1 - epsilon: #todo: study the certainty of this statement\n",
    "    print('No distribution shift detected')\n",
    "else:\n",
    "    print('Distribution shift detected')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can furthermore compute the p-value of the hypothesis that the shifted dataset is in the confidence interval of the full conformal model, using a binomial test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = binomtest(cummulative*len(X_shift), len(X_shift), 1-epsilon, alternative='greater')\n",
    "print('p-value: ', test.pvalue)\n",
    "print('Confidence interval: ', test.proportion_ci())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important question}: here we want to disprove that with probability $1-\\delta$ sampled from all possible training sets, the loss of the test set is larger than $\\lambda$ with probability $1-\\epsilon$. Would we have to run many binomial tests, get the pvalue of each of them and then compute the p-value over the hypothesis that the failure happens with probability $1-\\delta$?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_y, lower_bound, upper_bound"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that a simpler non-full conformal version of this test can be found in the appendix D of Angelopolous paper. A slightly modified version is the following: If $(X_i, Y_i)$ and $X_{test}, Y_{test}$ are iid, and we define\n",
    "$$\\hat{\\lambda} = \\inf\\left\\{\\lambda: \\frac{|\\{i: L(X, Y)\\leq \\lambda\\}|}{n}\\geq \\frac{\\lceil (n+1)(1-\\epsilon) \\rceil}{n}\\right\\}, $$\n",
    "the probability of $Y_{test}$ being in the conformal prediction set of $X_{test}$ is\n",
    "$$\\mathbb{P}(Y_{test}\\in \\mathcal{C}_{\\hat{\\lambda}}(X_{test})) \\geq 1-\\epsilon, $$\n",
    "where \n",
    "$$\\mathcal{C}_{\\hat{\\lambda}}(X_{test}) = \\{y: L(X,y)\\leq \\hat{\\lambda}\\}.$$\n",
    "Thus a simple binomial test can be used to compute the p-value of the hypothesis that the shifted dataset is in the confidence interval of the full conformal model. Eg, if $$\\mathbb{P}(Y_{test}\\in \\mathcal{C}_{\\hat{\\lambda}}(X_{test})) \\geq 1-\\epsilon$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time-stratified loss metric"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative easy way is to check whether the loss of in the new dataset is larger than in the original model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_hat = random_forest_model.predict(X_train)\n",
    "intervals_y_train = cr_std.predict(y_hat=y_hat_shift, confidence=1-delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_loss(y_hat, y_train, alpha, intervals, delta):\n",
    "    \"\"\" Computes the loss function for a given sample.\"\"\"\n",
    "    \n",
    "    sigma = compute_sigma(delta, intervals, y_train)\n",
    "    interval_alpha = confidence_interval(y_hat, sigma, alpha, [y_hat - sigma, y_hat + sigma])\n",
    "    return compute_loss(y_hat, y_train, sigma, interval_alpha)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "for j in tqdm(range(20)):\n",
    "    X_train_subset = X_train[j*485:(j+1)*485]\n",
    "    y_train_subset = y_train_hat[j*485:(j+1)*485]\n",
    "\n",
    "    y_train_hat_subset = random_forest_model.predict(X_train_subset)\n",
    "    intervals_y_train = cr_std.predict(y_hat=y_train_hat_subset, confidence=1-delta)\n",
    "    losses_ = []\n",
    "    for i in range(len(y_train_subset)):\n",
    "        losses_.append(quick_loss(y_train_hat_subset[i], y_train_subset[i], alpha, intervals_y_train[i], delta))\n",
    "\n",
    "    loss_train = np.mean(losses_)\n",
    "    losses.append(loss_train)\n",
    "\n",
    "y_shift_hat = random_forest_model.predict(X_shift)\n",
    "intervals_shift = cr_std.predict(y_hat=y_shift_hat, confidence=1-delta)\n",
    "loss_test = np.mean([quick_loss(y_shift_hat[i], y_shift[i], alpha, intervals_shift[i], delta) for i in range(len(y_shift))])\n",
    "\n",
    "if loss_test > np.quantile(losses, 0.95): #todo: change this by certainty level\n",
    "    print('Distribution shift detected')\n",
    "else:\n",
    "    print('No distribution shift detected')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conformal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e4bb16644b7805f2b6da05dca5aae8dc9248070802afd077ed9812795780bba0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
