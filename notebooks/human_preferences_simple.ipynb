{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conformal prediction from human preferences"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook I aim to explore how we can use conformal prediction to make model-free risk-controlled prediction from human preferences. We will start from a simple case study."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple case"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our simple case, we will consider input data of the form $x_1,\\ldots,x_d$ and will define a utility function $U(x_1,\\ldots,x_d) = u$ as a low degree polynomial function. For example a linear function. To make it simpler, we will restrict all the coefficients to be in the range $[-1/d,1/d]$, and the input features to be in the range $[-1,1]$, so that the output is between $-1$ and $1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def U(x, coefficients):\n",
    "    #coefficients /= np.linalg.norm(coefficients, ord = 1)\n",
    "    return sum(coefficients * x) % 2 - 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need a model to predict the utility function, or in other words, fit a model to replicate the behavior of $U(a)-U(b)$. The output of the model will be a softmax distribution over bins between $-1$ and $1$.\n",
    "We will follow Pytorch Lightning's [LightningModule](https://pytorch-lightning.readthedocs.io/en/stable/common/lightning_module.html) to define our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "class LitModel(pl.LightningModule):\n",
    "\n",
    "    \"\"\" PyTorch Lightning model.\n",
    "    Outputs the probability that model U(a,b) is in bin i.\n",
    "    \n",
    "    Args:\n",
    "        input_features (int): Number of input features of each of the two inputs.\n",
    "        output_predictions (int): Number of output prediction bins.\n",
    "        hidden_dim (int): Number of hidden units in the hidden layer.\n",
    "        layers (int): Number of hidden layers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_features, output_predictions, hidden_dim=128, layers = 1):\n",
    "        self.input_features = input_features\n",
    "        self.output_predictions = output_predictions\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layers = layers\n",
    "        super().__init__()\n",
    "\n",
    "        self.initial = nn.Sequential(\n",
    "            nn.Linear(2*self.input_features, self.hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.backbone_block = nn.Sequential(\n",
    "            nn.Linear(self.hidden_dim, self.hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(self.hidden_dim, self.output_predictions),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.initial(x)\n",
    "        for i in range(self.layers):\n",
    "            x = self.backbone_block(x)\n",
    "        x = self.head(x)\n",
    "        return x/(x.sum(dim=1).unsqueeze(1))\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.l1_loss(y_hat, y)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "def create_dataloader(x_list: list, y_list: list):\n",
    "    tensor_x = torch.Tensor(np.asarray(x_list)) # transform to torch tensor\n",
    "    tensor_y = torch.Tensor(np.asarray(y_list))\n",
    "    my_dataset = TensorDataset(tensor_x,tensor_y) # create your datset\n",
    "    return DataLoader(my_dataset, num_workers = 4) # create your dataloader\n",
    "\n",
    "def create_predict_dataloader(x_list: list):\n",
    "    tensor_x = torch.Tensor(np.asarray(x_list)) # transform to torch tensor\n",
    "    return DataLoader(tensor_x, num_workers = 4) # create your dataloader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The examples will be generated using the following function, which assigns them to bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-0.23667894,  0.15674154,  0.41631417, -0.11894899, -0.51452757,\n",
       "        -0.04644283]),\n",
       " array([ 0.22626756, -0.09963304,  0.01681321, -0.43850014,  0.46128857,\n",
       "        -0.2626279 ]),\n",
       " array([ 0.5239704 ,  0.22037693, -0.07098589, -0.11309304, -0.22264973,\n",
       "        -0.31765782]),\n",
       " array([-0.53670946,  0.22690499,  0.15271576, -0.21605744, -0.40273874,\n",
       "         0.10345592]),\n",
       " array([ 0.41519535,  0.43203526,  0.48316824, -0.67198386,  0.03727755,\n",
       "        -0.04733726]),\n",
       " array([ 0.37779181,  0.1335171 , -0.01670557, -0.50545807, -0.19671323,\n",
       "         0.17591687]),\n",
       " array([-0.1111115 ,  0.50739006,  0.40778453, -0.06110276, -0.15677432,\n",
       "        -0.35261553]),\n",
       " array([ 0.19243059, -0.15630802,  0.4487055 , -0.66165885, -0.59536609,\n",
       "         0.09727422]),\n",
       " array([ 0.05840442,  0.20977983,  0.15179187, -0.36763055, -0.56696353,\n",
       "        -0.27947376]),\n",
       " array([ 0.41965484,  0.60667182,  0.082924  , -0.42744416, -0.23043137,\n",
       "        -0.20951881])]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate random coefficients\n",
    "#coefficients = np.random.uniform(-1, 1, num_features)\n",
    "coefficients = np.array([0.5, 0.5, .2])\n",
    "\n",
    "def generate_examples(num_examples, num_features, num_bins, coefficients = coefficients):\n",
    "    \"\"\"Generates examples of human preferences\n",
    "    If we decide to use a binary loss function, it is sufficient with num_bins = 2.\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate random inputs\n",
    "    x0 = np.random.normal(loc = 0.25, scale = 0.3, size = (num_examples, num_features))\n",
    "    x1 = np.random.normal(loc = -0.25, scale = 0.3, size = (num_examples, num_features))\n",
    "\n",
    "    # Compute the utility of each input\n",
    "    u = np.array([U(x0[i], coefficients) - U(x1[i], coefficients) for i in range(num_examples)])\n",
    "\n",
    "    # Compute the bin of each input\n",
    "    bins = np.array([np.digitize(u[i], np.linspace(-1, 1, num_bins-1)) for i in range(num_examples)])\n",
    "\n",
    "    # Create the input list\n",
    "    x_list = []\n",
    "    for i in range(num_examples):\n",
    "        x_list.append(np.concatenate((x0[i], x1[i])))\n",
    "\n",
    "    # Create the output list\n",
    "    y_list = []\n",
    "    for i in range(num_examples):\n",
    "        y = np.zeros(num_bins)\n",
    "        y[bins[i]] = 1\n",
    "        y_list.append(y)\n",
    "\n",
    "    return x_list, y_list\n",
    "\n",
    "x, y = generate_examples(10, 3, 20, coefficients = np.array([0.5, 0.5, .2]))\n",
    "x[:10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then train a simple model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name           | Type       | Params\n",
      "----------------------------------------------\n",
      "0 | initial        | Sequential | 896   \n",
      "1 | backbone_block | Sequential | 16.5 K\n",
      "2 | head           | Sequential | 2.6 K \n",
      "----------------------------------------------\n",
      "20.0 K    Trainable params\n",
      "0         Non-trainable params\n",
      "20.0 K    Total params\n",
      "0.080     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be53df98fcf744acba4c6afe9907e8e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    }
   ],
   "source": [
    "num_examples = 1000\n",
    "num_features = 3\n",
    "num_bins = 20\n",
    "x_list, y_list = generate_examples(num_examples = num_examples, num_features = num_features, num_bins = num_bins)\n",
    "train_loader = create_dataloader(x_list, y_list)\n",
    "predict_loader = create_predict_dataloader(x_list)\n",
    "trainer = pl.Trainer(max_epochs=5)\n",
    "model = LitModel(input_features=num_features, output_predictions=num_bins)\n",
    "\n",
    "trainer.fit(model, train_dataloaders=train_loader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conformal prediction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that we defined the loss to be\n",
    "$$\n",
    "     \\mathcal{L}(U, a_i,b_i,y_i) =  y_i(U(a_i)-U(b_i)) + (1-y_i)(U(b_i)-U(a_i)),\n",
    "$$\n",
    "for $y_i \\in \\{0,1\\}$ the true preference, and $U$ the learned utility function."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the conformal prediction procedure we now follow the following steps:\n",
    "1. We have to define the set $\\mathcal{C}_\\alpha$:\n",
    "$$\n",
    "\\mathcal{C}_\\alpha(a_i,b_i) = \\{u_i = U(a_i)-U(b_i) \\in \\mathbb{R}: \\rho( u_i )\\geq  1-\\alpha \\}\n",
    "$$\n",
    "where $\\rho$ is the cumulative distribution function of the model's output distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def C(alpha: float, x_list: torch.Tensor):\n",
    "    loader = DataLoader(torch.Tensor(x_list))\n",
    "    predictions = trainer.predict(model,loader)\n",
    "    p = []\n",
    "    for prediction in predictions:\n",
    "        prediction = torch.flatten(prediction)\n",
    "        p.append(torch.where(prediction > alpha, torch.ones_like(prediction), torch.zeros_like(prediction)))\n",
    "    return torch.stack(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yw/g52bzl910kz1t3sdk0h_ydn80000gp/T/ipykernel_6618/1239800533.py:1: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:233.)\n",
      "  loader = DataLoader(torch.Tensor(x_list))\n",
      "/Users/pablo_1/opt/miniconda3/envs/conformal/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, predict_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62c929dee2b247f09666693e9dc65d4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 1000it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0.]]),\n",
       " tensor([[1.0000e+00, 7.6691e-25, 9.6473e-27, 2.9664e-25, 2.4066e-21, 1.5226e-26,\n",
       "          1.0534e-24, 4.5096e-26, 2.9778e-25, 2.0355e-25, 2.2128e-26, 1.4581e-21,\n",
       "          1.4540e-24, 3.0809e-21, 3.7551e-26, 1.1048e-25, 6.1125e-25, 1.1821e-26,\n",
       "          2.0940e-28, 1.4210e-25]]),\n",
       " tensor([[1.0000e+00, 2.9918e-30, 1.3230e-32, 9.3888e-31, 6.6670e-26, 2.3334e-32,\n",
       "          3.9491e-30, 9.4167e-32, 8.4926e-31, 5.8756e-31, 4.1166e-32, 3.1195e-26,\n",
       "          6.3448e-30, 7.1682e-26, 6.9774e-32, 2.6030e-31, 2.2062e-30, 1.7108e-32,\n",
       "          1.0669e-34, 3.6415e-31]]),\n",
       " tensor([[1.0000e+00, 1.2073e-36, 0.0000e+00, 2.6772e-37, 2.3462e-31, 3.1904e-39,\n",
       "          1.5786e-36, 1.6515e-38, 2.6184e-37, 1.5123e-37, 6.3095e-39, 7.0394e-32,\n",
       "          2.5056e-36, 2.2155e-31, 1.1581e-38, 6.0424e-38, 7.7258e-37, 0.0000e+00,\n",
       "          0.0000e+00, 7.9018e-38]]),\n",
       " tensor([[1.0000e+00, 6.6551e-24, 9.6939e-26, 3.0434e-24, 1.5611e-20, 1.6000e-25,\n",
       "          9.3374e-24, 4.8945e-25, 2.3996e-24, 1.8498e-24, 2.4940e-25, 1.1318e-20,\n",
       "          1.3927e-23, 1.9368e-20, 3.6173e-25, 1.0183e-24, 5.1448e-24, 1.1938e-25,\n",
       "          2.5578e-27, 1.4049e-24]]),\n",
       " tensor([[1.0000e+00, 2.3008e-34, 4.1275e-37, 5.8940e-35, 1.8761e-29, 9.3315e-37,\n",
       "          3.2073e-34, 4.1580e-36, 5.0899e-35, 2.8527e-35, 1.8191e-36, 5.7627e-30,\n",
       "          4.2445e-34, 1.6950e-29, 2.8120e-36, 1.4237e-35, 1.3191e-34, 6.7121e-37,\n",
       "          0.0000e+00, 1.6333e-35]]),\n",
       " tensor([[1.0000e+00, 3.0101e-30, 1.3017e-32, 9.3550e-31, 6.6551e-26, 2.3852e-32,\n",
       "          4.1103e-30, 9.0835e-32, 8.5580e-31, 5.7957e-31, 4.1567e-32, 2.8934e-26,\n",
       "          5.9484e-30, 7.1511e-26, 6.9895e-32, 2.7278e-31, 2.1752e-30, 1.8199e-32,\n",
       "          1.0847e-34, 3.4700e-31]]),\n",
       " tensor([[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 5.5216e-34, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.1470e-34,\n",
       "          0.0000e+00, 4.2781e-34, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00]]),\n",
       " tensor([[1.0000e+00, 2.8047e-31, 8.6539e-34, 7.1054e-32, 6.7858e-27, 1.8133e-33,\n",
       "          3.7870e-31, 6.7025e-33, 8.1817e-32, 3.6728e-32, 2.9480e-33, 1.8931e-27,\n",
       "          4.1722e-31, 5.9597e-27, 4.7929e-33, 2.0901e-32, 1.5258e-31, 1.3497e-33,\n",
       "          6.2676e-36, 2.2516e-32]]),\n",
       " tensor([[1.0000e+00, 3.3479e-29, 1.4889e-31, 1.0993e-29, 4.4808e-25, 3.1237e-31,\n",
       "          4.5762e-29, 1.1364e-30, 8.7957e-30, 5.3459e-30, 5.8453e-31, 1.7722e-25,\n",
       "          5.6961e-29, 4.2612e-25, 7.8243e-31, 3.2151e-30, 1.9597e-29, 2.4209e-31,\n",
       "          1.5951e-33, 3.5329e-30]])]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = DataLoader(torch.Tensor(np.asarray(x_list)))\n",
    "predictions = trainer.predict(model,loader)\n",
    "predictions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0.]),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "        0., 0., 0.]),\n",
       " array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0.]),\n",
       " array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0.]),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0.]),\n",
       " array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0.]),\n",
       " array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0.]),\n",
       " array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0.]),\n",
       " array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0.]),\n",
       " array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0.])]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75ad81901b62442eafba60fc2def5fb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 1000it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [1., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = C(0.5, x_list)\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('conformal')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e4bb16644b7805f2b6da05dca5aae8dc9248070802afd077ed9812795780bba0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
