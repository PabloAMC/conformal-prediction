{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conformal prediction from human preferences"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook I aim to explore how we can use conformal prediction to make model-free risk-controlled prediction from human preferences. We will start from a simple case study."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple case"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our simple case, we will consider input data of the form $x_1,\\ldots,x_d$ and will define a utility function $U(x_1,\\ldots,x_d) = u$ as a low degree polynomial function. For example a linear function. To make it simpler, we will restrict all the coefficients to be in the range $[-1/d,1/d]$, and the input features to be in the range $[-1,1]$, so that the output is between $-1$ and $1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def U(x, coefficients):\n",
    "    result = 0\n",
    "    for i in range(len(coefficients)):\n",
    "        result += coefficients[i] * x[i]\n",
    "    return result/len(coefficients)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need a model to predict the utility function, or in other words, fit a model to replicate the behavior of $U(a)-U(b)$. The output of the model will be a softmax distribution over bins between $-1$ and $1$.\n",
    "We will follow Pytorch Lightning's [LightningModule](https://pytorch-lightning.readthedocs.io/en/stable/common/lightning_module.html) to define our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "class LitModel(pl.LightningModule):\n",
    "\n",
    "    \"\"\" PyTorch Lightning model.\n",
    "    Outputs the probability that model U(a,b) is in bin i.\n",
    "    \n",
    "    Args:\n",
    "        input_features (int): Number of input features of each of the two inputs.\n",
    "        output_predictions (int): Number of output prediction bins.\n",
    "        hidden_dim (int): Number of hidden units in the hidden layer.\n",
    "        layers (int): Number of hidden layers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_features, output_predictions, hidden_dim=128, layers = 3):\n",
    "        self.input_features = input_features\n",
    "        self.output_predictions = output_predictions\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layers = layers\n",
    "        super().__init__()\n",
    "\n",
    "        self.initial = nn.Sequential(\n",
    "            nn.Linear(2*self.input_features, self.hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.backbone_block = nn.Sequential(\n",
    "            nn.Linear(self.hidden_dim, self.hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(self.hidden_dim, self.output_predictions),\n",
    "            nn.Softmax()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.initial(x)\n",
    "        for i in range(self.layers):\n",
    "            x = self.backbone_block(x)\n",
    "        return self.head(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.kl_div(y_hat, y)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "def create_dataloader(x_list: list, y_list: list):\n",
    "    tensor_x = torch.Tensor(x_list) # transform to torch tensor\n",
    "    tensor_y = torch.Tensor(y_list)\n",
    "    my_dataset = TensorDataset(tensor_x,tensor_y) # create your datset\n",
    "    return DataLoader(my_dataset) # create your dataloader\n",
    "\n",
    "def create_predict_dataloader(x_list: list):\n",
    "    tensor_x = torch.Tensor(x_list) # transform to torch tensor\n",
    "    return DataLoader(tensor_x) # create your dataloader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The examples will be generated using the following function, which assigns them to bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_examples(num_examples, num_features, num_bins):\n",
    "    \"\"\"Generates examples of human preferences\n",
    "    If we decide to use a binary loss function, it is sufficient with num_bins = 2.\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate random coefficients\n",
    "    coefficients = np.random.uniform(-1, 1, num_features)\n",
    "\n",
    "    # Generate random inputs\n",
    "    x0 = np.random.uniform(-1, 1, (num_examples, num_features))\n",
    "    x1 = np.random.uniform(-1, 1, (num_examples, num_features))\n",
    "\n",
    "    # Compute the utility of each input\n",
    "    u = np.array([U(x0[i], coefficients) - U(x1[i], coefficients) for i in range(num_examples)])\n",
    "\n",
    "    # Compute the bin of each input\n",
    "    bins = np.array([np.digitize(u[i], np.linspace(-1, 1, num_bins)) for i in range(num_examples)])\n",
    "\n",
    "    # Create the input list\n",
    "    x_list = []\n",
    "    for i in range(num_examples):\n",
    "        x_list.append(np.concatenate((x0[i], x1[i])))\n",
    "\n",
    "    # Create the output list\n",
    "    y_list = []\n",
    "    for i in range(num_examples):\n",
    "        y = np.zeros(num_bins)\n",
    "        y[bins[i]] = 1\n",
    "        y_list.append(y)\n",
    "\n",
    "    return x_list, y_list\n",
    "\n",
    "#generate_examples(10, 2, 20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then train a simple model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yw/g52bzl910kz1t3sdk0h_ydn80000gp/T/ipykernel_6822/273449737.py:6: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:233.)\n",
      "  tensor_x = torch.Tensor(x_list) # transform to torch tensor\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name           | Type       | Params\n",
      "----------------------------------------------\n",
      "0 | initial        | Sequential | 896   \n",
      "1 | backbone_block | Sequential | 16.5 K\n",
      "2 | head           | Sequential | 2.6 K \n",
      "----------------------------------------------\n",
      "20.0 K    Trainable params\n",
      "0         Non-trainable params\n",
      "20.0 K    Total params\n",
      "0.080     Total estimated model params size (MB)\n",
      "/Users/pablo_1/opt/miniconda3/envs/conformal/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78e7dd5e9f654e668bf06ab3e8daa49c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pablo_1/opt/miniconda3/envs/conformal/lib/python3.10/site-packages/torch/nn/modules/container.py:204: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n",
      "/Users/pablo_1/opt/miniconda3/envs/conformal/lib/python3.10/site-packages/torch/nn/functional.py:2916: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\n",
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    }
   ],
   "source": [
    "num_examples = 1000\n",
    "num_features = 3\n",
    "num_bins = 20\n",
    "x_list, y_list = generate_examples(num_examples = num_examples, num_features = num_features, num_bins = num_bins)\n",
    "train_loader = create_dataloader(x_list, y_list)\n",
    "predict_loader = create_predict_dataloader(x_list)\n",
    "trainer = pl.Trainer(max_epochs=5)\n",
    "model = LitModel(input_features=num_features, output_predictions=num_bins)\n",
    "\n",
    "trainer.fit(model, train_dataloaders=train_loader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conformal prediction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that we defined the loss to be\n",
    "$$\n",
    "     \\mathcal{L}(U, a_i,b_i,y_i) =  y_i(U(a_i)-U(b_i)) + (1-y_i)(U(b_i)-U(a_i)),\n",
    "$$\n",
    "for $y_i \\in \\{0,1\\}$ the true preference, and $U$ the learned utility function."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the conformal prediction procedure we now follow the following steps:\n",
    "1. We have to define the set $\\mathcal{C}_\\alpha$:\n",
    "$$\n",
    "\\mathcal{C}_\\alpha(a_i,b_i) = \\{u_i = U(a_i)-U(b_i) \\in \\mathbb{R}: \\rho( u_i )\\geq  1-\\alpha \\}\n",
    "$$\n",
    "where $\\rho$ is the cumulative distribution function of the model's output distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def C(alpha: float, x_list: torch.Tensor):\n",
    "    loader = DataLoader(torch.Tensor(x_list))\n",
    "    predictions = trainer.predict(model,loader)\n",
    "    p = []\n",
    "    for prediction in predictions:\n",
    "        prediction = torch.flatten(prediction)\n",
    "        p.append(torch.where(prediction > alpha, torch.ones_like(prediction), torch.zeros_like(prediction)))\n",
    "    return torch.stack(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pablo_1/opt/miniconda3/envs/conformal/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, predict_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd548d4643364b018c2c3e193db6c6e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 1000it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pablo_1/opt/miniconda3/envs/conformal/lib/python3.10/site-packages/torch/nn/modules/container.py:204: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = C(0.5, x_list)\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('conformal')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e4bb16644b7805f2b6da05dca5aae8dc9248070802afd077ed9812795780bba0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
