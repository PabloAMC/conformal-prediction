{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting distribution shifts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(0)\n",
    "from scipy.optimize import fsolve\n",
    "from scipy.special import erf\n",
    "from scipy.integrate import quad\n",
    "from functools import partial\n",
    "from scipy.stats import binomtest\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_openml\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "\n",
    "from crepes import ConformalRegressor, ConformalPredictiveSystem\n",
    "\n",
    "from crepes.fillings import (sigma_variance, \n",
    "                            sigma_variance_oob,\n",
    "                            sigma_knn,\n",
    "                            binning)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we explain how to implement the shift detection algorithm. We start downloading the data and modeling it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pablo_1/opt/miniconda3/envs/conformal/lib/python3.10/site-packages/sklearn/datasets/_openml.py:932: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before normalization range: 75000.0 7700000.0\n",
      "After normalization range: 0.13798043023679007 0.9999999999991728\n"
     ]
    }
   ],
   "source": [
    "# First we load the dataset\n",
    "dataset = fetch_openml(name=\"house_sales\",version=3)\n",
    "\n",
    "X = dataset.data.values.astype(float)\n",
    "y = dataset.target.values.astype(float)\n",
    "print('Before normalization range:',y.min(), y.max())\n",
    "\n",
    "# and also normalize it such that the y values are in the range [0,1]\n",
    "#y = (np.tanh(np.array([(y[i]-y.min())/(y.max()-y.min()) for i in range(len(y))])*2-1)+1)/2.\n",
    "#y = np.array([(y[i]-y.min())/(y.max()-y.min()) for i in range(len(y))])\n",
    "y = np.tanh(y/y.mean())\n",
    "print('After normalization range:',y.min(), y.max())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we spit the data in the main and shifted data, and form tuples for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After creating tuples (10202, 42) (604, 42)\n",
      "(10202,)\n",
      "(604,)\n"
     ]
    }
   ],
   "source": [
    "# Then we split the data such that we can later on introduce a distribution shift\n",
    "X_main, X_shift, y_main, y_shift = X[(4e4 >= X[:, 3])] , X[(4e4 < X[:, 3])] , y[(4e4 >= X[:, 3])] , y[(4e4 < X[:, 3])]\n",
    "\n",
    "# We want to have an even number of samples, to create tuples\n",
    "if len(X_shift) % 2 == 1:\n",
    "    X_shift = X_shift[:-1]\n",
    "    y_shift = y_shift[:-1]\n",
    "if len(X_main) % 2 == 1:\n",
    "    X_main = X_main[:-1]\n",
    "    y_main = y_main[:-1]\n",
    "\n",
    "## The input will be two copies of X, one for each house\n",
    "X_main = X_main.reshape(X_main.shape[0]//2,-1)\n",
    "X_shift = X_shift.reshape(X_shift.shape[0]//2,-1)\n",
    "print('After creating tuples',X_main.shape, X_shift.shape)\n",
    "\n",
    "## We want to estimate the difference between the two y values\n",
    "y_main = y_main.reshape(y_main.shape[0]//2,-1)\n",
    "y_main = y_main[:,0] - y_main[:,1]\n",
    "print(y_main.shape)\n",
    "\n",
    "## and also in the shifted dataset\n",
    "y_shift = y_shift.reshape(y_shift.shape[0]//2,-1)\n",
    "y_shift = y_shift[:,0] - y_shift[:,1]\n",
    "print(y_shift.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we create a calibration dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_cal, y_train, y_cal = train_test_split(X_main, y_main, test_size=500)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create the model and train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestRegressor(n_estimators=500, n_jobs=-1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(n_estimators=500, n_jobs=-1)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestRegressor(n_estimators=500, n_jobs=-1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Then we train a random forest regressor on the training set\n",
    "random_forest_model = RandomForestRegressor(n_jobs=-1, n_estimators=500) \n",
    "random_forest_model.fit(X_train, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conformal model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will target a success probability $1-\\delta = 0.95$. Then we train the conformal model on the residuals on the predictions of the random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConformalRegressor(fitted=True, normalized=False, mondrian=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta = 0.05\n",
    "\n",
    "cr_std = ConformalRegressor()\n",
    "y_hat_cal = random_forest_model.predict(X_cal)\n",
    "residuals_cal = y_cal - y_hat_cal\n",
    "# and fit it to the residuals\n",
    "cr_std.fit(residuals=residuals_cal)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now make prediction on a given test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using such model we can now predict the residuals on the test set\n",
    "y_hat_shift = random_forest_model.predict(X_shift)\n",
    "intervals_std = cr_std.predict(y_hat=y_hat_shift, confidence=1-delta)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional -- More complex models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous models assigns the same uncertainty interval to all predictions. We can alternatively use two more models that callibrate the confidence interval for each prediction based on their heuristic difficulty using k-nearest neighbours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second model based on k-nn\n",
    "sigmas_cal_knn = sigma_knn(X=X_cal, residuals=residuals_cal)\n",
    "cr_norm_knn = ConformalRegressor()\n",
    "cr_norm_knn.fit(residuals=residuals_cal, sigmas=sigmas_cal_knn)\n",
    "sigmas_test_knn = sigma_knn(X=X_cal, residuals=residuals_cal, X_test=X_shift)\n",
    "intervals_norm_knn = cr_norm_knn.predict(y_hat=y_hat_shift, \n",
    "                                        sigmas=sigmas_test_knn,\n",
    "                                        y_min=0, y_max=1)\n",
    "\n",
    "# Third model based on binning\n",
    "bins_cal, bin_thresholds = binning(values=sigmas_cal_knn, bins=20)\n",
    "cr_mond = ConformalRegressor()\n",
    "cr_mond.fit(residuals=residuals_cal, bins=bins_cal)\n",
    "bins_test = binning(values=sigmas_test_knn, bins=bin_thresholds)\n",
    "intervals_mond = cr_mond.predict(y_hat=y_hat_shift, bins=bins_test, \n",
    "                                        y_min=0, y_max=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compute the coverage of the three models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Coverage</th>\n",
       "      <th>Mean size</th>\n",
       "      <th>Median size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Std CR</th>\n",
       "      <td>0.8791</td>\n",
       "      <td>0.4294</td>\n",
       "      <td>0.4294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Norm CR knn</th>\n",
       "      <td>0.4603</td>\n",
       "      <td>0.2744</td>\n",
       "      <td>0.2657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mond CR</th>\n",
       "      <td>0.4354</td>\n",
       "      <td>0.2328</td>\n",
       "      <td>0.2333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mean</th>\n",
       "      <td>0.5916</td>\n",
       "      <td>0.3122</td>\n",
       "      <td>0.3095</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Coverage  Mean size  Median size\n",
       "Std CR         0.8791     0.4294       0.4294\n",
       "Norm CR knn    0.4603     0.2744       0.2657\n",
       "Mond CR        0.4354     0.2328       0.2333\n",
       "Mean           0.5916     0.3122       0.3095"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "coverages = []\n",
    "mean_sizes = []\n",
    "median_sizes = []\n",
    "\n",
    "prediction_intervals = {\n",
    "    \"Std CR\":intervals_std,\n",
    "    \"Norm CR knn\":intervals_norm_knn,\n",
    "    \"Mond CR\":intervals_mond,\n",
    "}\n",
    "\n",
    "for name in prediction_intervals.keys():\n",
    "    intervals = prediction_intervals[name]\n",
    "    coverages.append(np.sum([1 if (y_shift[i]>=intervals[i,0] and \n",
    "                                   y_shift[i]<=intervals[i,1]) else 0 \n",
    "                            for i in range(len(y_shift))])/len(y_shift))\n",
    "    mean_sizes.append((intervals[:,1]-intervals[:,0]).mean())\n",
    "    median_sizes.append(np.median((intervals[:,1]-intervals[:,0])))\n",
    "\n",
    "pred_int_df = pd.DataFrame({\"Coverage\":coverages, \n",
    "                            \"Mean size\":mean_sizes, \n",
    "                            \"Median size\":median_sizes}, \n",
    "                           index=list(prediction_intervals.keys()))\n",
    "\n",
    "pred_int_df.loc[\"Mean\"] = [pred_int_df[\"Coverage\"].mean(), \n",
    "                           pred_int_df[\"Mean size\"].mean(),\n",
    "                           pred_int_df[\"Median size\"].mean()]\n",
    "\n",
    "display(pred_int_df.round(4))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auxiliary functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous section we have used pre-defined conformal models to compute the confidence intervals. We can also assume gaussian intervals derived from these residuals and reverse engineer the confidence intervals."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we define function that computes the standard deviation from an interval and $1-\\delta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sigma(delta, initial_interval, mean):\n",
    "    \"\"\" Computes the standard deviation of a normal distribution such that the probability of the interval is 1-delta\"\"\"\n",
    "    def cdf(sigma_):\n",
    "        return 0.5*(erf((initial_interval[1]-mean)/(np.sqrt(2)*sigma_))-erf((initial_interval[0]-mean)/(np.sqrt(2)*sigma_))) - (1-delta)\n",
    "\n",
    "    sigma = fsolve(cdf, (initial_interval[1]-initial_interval[0])/2)[0]\n",
    "    return sigma"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we define a function to compute the interval in which the (assumed normal) probability distribution is larger than $\\alpha$. In other words, $\\mathcal{C}_\\alpha$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confidence_interval(mean, sigma, alpha, initial_interval_guess):\n",
    "    \"\"\" We want to compute the conformal integral in which the probability density is larger than alpha. \"\"\"\n",
    "    def normal_distribution(x, mean, sigma):\n",
    "        return np.exp(-((x-mean)**2)/(2*sigma**2))\n",
    "\n",
    "    interval_alpha = fsolve(lambda x: normal_distribution(x, mean, sigma) - alpha, x0 = initial_interval_guess)\n",
    "    return interval_alpha"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we define a function to integrate the loss on a given interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(y_hat, y_gt, sigma, interval_alpha):\n",
    "    \"\"\" Integrates the  quadratic loss over the interval defined by the probability density being larger than alpha.\"\"\"\n",
    "    def normal_loss(x, y, mean, sigma):\n",
    "        return np.exp(-((x-mean)**2)/(2*sigma**2))*np.abs(x-y)\n",
    "\n",
    "    normal_l = partial(normal_loss, y=y_gt, mean=y_hat, sigma = sigma)\n",
    "    loss = quad(normal_l, interval_alpha[0], interval_alpha[1])[0]\n",
    "    return loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to compute the p-value of the hypothesis $\\mathcal{H}_\\alpha$ that the risk $R(\\alpha)\\leq \\lambda$ for a chosen $\\lambda$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_ = 0.5\n",
    "\n",
    "def pvalue(y_gt, y_hat, sigma, lambda_):\n",
    "    \"\"\" Computes the p-value of the hypothesis that the risk is lower than some \\lambda.\"\"\"\n",
    "\n",
    "    # For each y compute confidence interval first compute the loss\n",
    "    alpha = np.linspace(0.01, 0.99, 100)\n",
    "    interval_alpha = confidence_interval(mean = y_hat, sigma = sigma, alpha = alpha, initial_interval_guess=[y_hat - sigma, y_hat+sigma])\n",
    "    loss = compute_loss(y_hat, y_gt, sigma, interval_alpha)\n",
    "\n",
    "    # Step 1: Compute p-values\n",
    "    pvalues = np.exp(-2*len(y_hat)*(lambda_ - loss)**2)\n",
    "\n",
    "    # Step 2: Family-wise error correction\n",
    "    reject, pvals_corrected, _, bonferroni_delta = multipletests(pvalues, delta, method = 'bonferroni')\n",
    "\n",
    "    return pvals_corrected, reject"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full conformal prediction and inductive conformal predictors\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to compute the loss of exchanging one element of the training set with the shifted data. Then we will compute the corresponding p-values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_x = np.expand_dims(X_shift[0], axis = 0)\n",
    "new_y = np.expand_dims(y_shift[0], axis = 0)\n",
    "\n",
    "alpha = 1e-5\n",
    "ncal = len(y_cal)\n",
    "\n",
    "# First we fit a model to the data\n",
    "random_forest_model = RandomForestRegressor(n_estimators=100)\n",
    "random_forest_model.fit(X_train, y_train)\n",
    "\n",
    "def find_lamdas(new_x, new_y, delta, random_forest_model: RandomForestRegressor, X_cal, y_cal):\n",
    "    lambdas_i = []\n",
    "    ncal = len(X_cal)\n",
    "    for i in range(ncal + 1):\n",
    "        if i < ncal:\n",
    "            j = np.random.randint(0, len(X_cal))\n",
    "        \n",
    "            X_cal_j = np.concatenate([X_cal[:j],new_x, X_cal[j+1:]])\n",
    "            y_cal_j = np.concatenate([y_cal[:j],new_y, y_cal[j+1:]])\n",
    "\n",
    "            X_test_j = np.expand_dims(X_cal[j], axis = 0)\n",
    "            y_test_j = np.expand_dims(y_cal[j], axis = 0)\n",
    "\n",
    "        else:\n",
    "            X_cal_j = X_cal\n",
    "            y_cal_j = y_cal\n",
    "\n",
    "            X_test_j = new_x\n",
    "            y_test_j = new_y\n",
    "\n",
    "        # We can now compute the residuals\n",
    "        y_hat_cal_j = random_forest_model.predict(X_cal_j)\n",
    "        residuals_cal = y_cal_j - y_hat_cal_j\n",
    "\n",
    "        # We can now fit a model to the residuals\n",
    "        cr_std = ConformalRegressor()\n",
    "        cr_std.fit(residuals=residuals_cal)\n",
    "\n",
    "        # Compute confidence intervals on sample X_train[j]\n",
    "        y_hat_j = random_forest_model.predict(X_test_j)\n",
    "        interval_j_std = cr_std.predict(y_hat=y_hat_j, confidence=1-delta)[0]\n",
    "\n",
    "        # Compute loss\n",
    "        sigma = compute_sigma(delta = delta, initial_interval = interval_j_std, mean = y_hat_j)\n",
    "        #interval_alpha = confidence_interval(mean = y_hat_j, sigma = sigma, alpha = alpha, initial_interval_guess = interval_j_std)\n",
    "        interval_alpha = interval_j_std\n",
    "        lambda_i = compute_loss(y_hat_j, y_test_j, sigma, interval_alpha)\n",
    "\n",
    "        # Compute lambda_i\n",
    "        lambdas_i.append(lambda_i)\n",
    "\n",
    "        # Save the last sigma, corresponding to the new sample, which we will use later on.\n",
    "        if i == ncal: \n",
    "            sigma_new = sigma\n",
    "            interval_alpha_new = interval_alpha\n",
    "\n",
    "    lambdas_i, lambda_y = lambdas_i[:-1], lambdas_i[-1]\n",
    "\n",
    "    return lambdas_i, lambda_y, sigma_new, interval_alpha_new\n",
    "\n",
    "lambdas_i, lambda_y, sigma_new, interval_alpha_new = find_lamdas(new_x, new_y, delta, random_forest_model, X_cal, y_cal)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to compute the possible values of y such that the corresponding p-value is larger than $\\epsilon$, $p^y \\geq \\epsilon$ for \n",
    "$$p^y := \\frac{|\\{ i \\in 0,\\ldots,n_{cal}| \\lambda_i \\leq \\lambda_y\\}|+1}{n_{cal}+1}\\geq \\epsilon.$$\n",
    "First we compute how the p-value should be\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We do not detect distribution shift, based on the p-value above\n"
     ]
    }
   ],
   "source": [
    "epsilon = 0.05\n",
    "lambda_y_lower_bound = np.quantile(lambdas_i, (epsilon*(ncal + 1) - 1)/len(lambdas_i))\n",
    "if lambda_y < lambda_y_lower_bound:\n",
    "    print(\"We detect distribution shift, based on the p-value above\")\n",
    "else:\n",
    "    print(\"We do not detect distribution shift, based on the p-value above\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to find the values of y that satisfies such bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_loss_partial = partial(compute_loss, y_gt = new_y, sigma = sigma_new, interval_alpha = interval_alpha_new)\n",
    "\n",
    "y_lower_bound = fsolve(lambda x: compute_loss_partial(x) - lambda_y_lower_bound, x0 = new_y-0.5)\n",
    "y_upper_bound = fsolve(lambda x: compute_loss_partial(x) - lambda_y_lower_bound, x0 = new_y+0.5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice we want to compute, for each value of x in the shifted dataset, the corresponding bounds on y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bounds(x, y, epsilon, delta, delta_prime, X_cal, Y_cal, model):\n",
    "    \"\"\" Computes the bounds on y for a given x.\"\"\"\n",
    "    # Compute the lambda_i values in the full conformal model\n",
    "    find_lamdas_partial = partial(find_lamdas, delta = delta, random_forest_model = random_forest_model, X_cal = X_cal, y_cal = y_cal)\n",
    "    lambdas_i, lambda_y, sigma_new, interval_alpha_new = find_lamdas_partial(x, y)\n",
    "\n",
    "    # Compute the lower bound on lambda_y\n",
    "    ncal = len(X_cal)\n",
    "    epsilon = epsilon + np.sqrt(-np.log(delta_prime)/(2*ncal))\n",
    "    lambda_y_lower_bound = np.quantile(lambdas_i, (epsilon*(ncal + 1) - 1)/len(lambdas_i))\n",
    "\n",
    "    # Compute the bounds on y\n",
    "    compute_loss_partial = partial(compute_loss, y_gt = y, sigma = sigma_new, interval_alpha = interval_alpha_new)\n",
    "    y_lower_bound = fsolve(lambda x: compute_loss_partial(x) - lambda_y_lower_bound, x0 = y-0.5) #todo: this does not seem to work very well\n",
    "    y_upper_bound = fsolve(lambda x: compute_loss_partial(x) - lambda_y_lower_bound, x0 = y+0.5)\n",
    "\n",
    "    return y_lower_bound, y_upper_bound\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can check whether the shifted dataset is in the confidence interval of the full conformal model with probability $1-\\epsilon$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_shift, y_shift = X_shift[:10], y_shift[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [10:11, 61.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution shift detected\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "cummulative = 0\n",
    "delta_prime = 1/2**4 # This is the delta in the definition of the inductive conformal predictor.\n",
    "delta = 1/2**4 # This is the delta used in the for the estimation of uncertainty intervals in the conformal model, to integrate the loss\n",
    "\n",
    "for new_x, new_y in tqdm(zip(X_shift, y_shift)):\n",
    "    new_x, new_y = np.expand_dims(new_x, axis = 0), np.expand_dims(new_y, axis = 0)\n",
    "    lower_bound, upper_bound = compute_bounds(new_x, new_y, epsilon, delta, delta_prime, X_cal, y_cal, random_forest_model)\n",
    "    if new_y > lower_bound and new_y < upper_bound: cummulative += 1\n",
    "\n",
    "cummulative /= len(X_shift)\n",
    "if cummulative > 1 - epsilon: #todo: study the certainty of this statement\n",
    "    print('No distribution shift detected')\n",
    "else:\n",
    "    print('Distribution shift detected')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can furthermore compute the p-value of the hypothesis that the shifted dataset is in the confidence interval of the full conformal model, using a binomial test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p-value:  1.0\n",
      "Confidence interval:  ConfidenceInterval(low=0.0, high=1.0)\n"
     ]
    }
   ],
   "source": [
    "test = binomtest(int(cummulative*len(X_shift)), len(X_shift), 1-epsilon, alternative='greater')\n",
    "print('p-value: ', test.pvalue)\n",
    "print('Confidence interval: ', test.proportion_ci())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important question}: here we want to disprove that with probability $1-\\delta$ sampled from all possible training sets, the loss of the test set is larger than $\\lambda$ with probability $1-\\epsilon$. Would we have to run many binomial tests, get the pvalue of each of them and then compute the p-value over the hypothesis that the failure happens with probability $1-\\delta$?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_y, lower_bound, upper_bound"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that a simpler non-full conformal version of this test can be found in the appendix D of Angelopolous paper. A slightly modified version is the following: If $(X_i, Y_i)$ and $X_{test}, Y_{test}$ are iid, and we define\n",
    "$$\\hat{\\lambda} = \\inf\\left\\{\\lambda: \\frac{|\\{i: L(X, Y)\\leq \\lambda\\}|}{n}\\geq \\frac{\\lceil (n+1)(1-\\epsilon) \\rceil}{n}\\right\\}, $$\n",
    "the probability of $Y_{test}$ being in the conformal prediction set of $X_{test}$ is\n",
    "$$\\mathbb{P}(Y_{test}\\in \\mathcal{C}_{\\hat{\\lambda}}(X_{test})) \\geq 1-\\epsilon, $$\n",
    "where \n",
    "$$\\mathcal{C}_{\\hat{\\lambda}}(X_{test}) = \\{y: L(X,y)\\leq \\hat{\\lambda}\\}.$$\n",
    "Thus a simple binomial test can be used to compute the p-value of the hypothesis that the shifted dataset is in the confidence interval of the full conformal model. Eg, if $$\\mathbb{P}(Y_{test}\\in \\mathcal{C}_{\\hat{\\lambda}}(X_{test})) \\geq 1-\\epsilon$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time-stratified loss metric"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative easy way is to check whether the loss of in the new dataset is larger than in the original model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_hat = random_forest_model.predict(X_train)\n",
    "intervals_y_train = cr_std.predict(y_hat=y_hat_shift, confidence=1-delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_loss(y_hat, y_train, alpha, intervals, delta):\n",
    "    \"\"\" Computes the loss function for a given sample.\"\"\"\n",
    "    \n",
    "    sigma = compute_sigma(delta, intervals, y_train)\n",
    "    interval_alpha = confidence_interval(y_hat, sigma, alpha, [y_hat - sigma, y_hat + sigma])\n",
    "    return compute_loss(y_hat, y_train, sigma, interval_alpha)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:18<00:00,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution shift detected\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/Users/pablo_1/opt/miniconda3/envs/conformal/lib/python3.10/site-packages/scipy/optimize/_minpack_py.py:175: RuntimeWarning: The iteration is not making good progress, as measured by the \n",
      "  improvement from the last ten iterations.\n",
      "  warnings.warn(msg, RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "for j in tqdm(range(20)):\n",
    "    X_train_subset = X_train[j*485:(j+1)*485]\n",
    "    y_train_subset = y_train_hat[j*485:(j+1)*485]\n",
    "\n",
    "    y_train_hat_subset = random_forest_model.predict(X_train_subset)\n",
    "    intervals_y_train = cr_std.predict(y_hat=y_train_hat_subset, confidence=1-delta)\n",
    "    losses_ = []\n",
    "    for i in range(len(y_train_subset)):\n",
    "        losses_.append(quick_loss(y_train_hat_subset[i], y_train_subset[i], alpha, intervals_y_train[i], delta))\n",
    "\n",
    "    loss_train = np.mean(losses_)\n",
    "    losses.append(loss_train)\n",
    "\n",
    "y_shift_hat = random_forest_model.predict(X_shift)\n",
    "intervals_shift = cr_std.predict(y_hat=y_shift_hat, confidence=1-delta)\n",
    "loss_test = np.mean([quick_loss(y_shift_hat[i], y_shift[i], alpha, intervals_shift[i], delta) for i in range(len(y_shift))])\n",
    "\n",
    "if loss_test > np.quantile(losses, 0.95): #todo: change this by certainty level\n",
    "    print('Distribution shift detected')\n",
    "else:\n",
    "    print('No distribution shift detected')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13951661722836595"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.024523803530562934,\n",
       " 0.02452380353056293,\n",
       " 0.024523803530562934,\n",
       " 0.024523803530562934,\n",
       " 0.02452380353056293,\n",
       " 0.024523803530562934,\n",
       " 0.02452380353056293,\n",
       " 0.02452380353056293,\n",
       " 0.024523803530562934,\n",
       " 0.02452380353056293,\n",
       " 0.024523803530562934,\n",
       " 0.02452380353056293,\n",
       " 0.024523803530562934,\n",
       " 0.02452380353056293,\n",
       " 0.02452380353056293,\n",
       " 0.02452380353056293,\n",
       " 0.02452380353056293,\n",
       " 0.02452380353056293,\n",
       " 0.02452380353056293,\n",
       " 0.02452380353056293]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conformal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e4bb16644b7805f2b6da05dca5aae8dc9248070802afd077ed9812795780bba0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
